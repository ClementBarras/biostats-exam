---
title: "Examen de biostatistiques"
author: "Barras Clément & Delebarre Bertille"
date: "April 1, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE)
```

## Chargement du jeu de données `prostate.txt`

```{r cars}
library(factoextra)
library(glmnet)
prostate <- read.delim("../data/prostate.txt", sep=" ")
X = as.matrix(prostate[, -1])
y = as.matrix(prostate$y)
n <- nrow(X)
p <- ncol(X)
```

## Question 1.

On effectue une ACP sur les données, et on les représente sur le premier plan principal :

```{r}
fit.pca = prcomp(X, scale = TRUE, center = TRUE)
fviz_pca_ind(fit.pca, col.ind = y)
```

Le premier axe principal explique plus de $40\%$ de la variance, ce qui est remarquable au vu du nombre de variables.
Cependant, les deux catégories d'individus ne sont pas séparable avec cette projection.

La variable `fit.pca$rotation` contient les vecteurs propres de $\bar{X}^T\bar{X}$ où $\bar{X}$ désigne la matrice des échantillons centrée et réduite.

Afin de représenter quels gènes contribuent le plus aux première et deuxième composantes, on trie les coordonnées des colonnes correspondantes par ordre décroissant en valeur absolue :

```{r}
sort(abs(fit.pca$rotation[,1]), decreasing = TRUE)[1:10]
```
```{r}
sort(abs(fit.pca$rotation[,2]), decreasing = TRUE)[1:10]
```

## Question 2.

Dans cette partie, nous séparons le jeu de données en deux parties : les individus sains et les individus malades.

```{r}
Xsain = X[1:50,]
Xmalade = X[51:102,]
```

Nous effectuons un test de Student sur chacune des variables afin de comparer par leur moyenne les échantillons sains et les échantillons malades sur cette variable. Selon l'hypothèse $\mathcal{H}_0$, la variable correspond à un gène qui ne s'exprime pas différentiellement entre patients et contrôles. Selon l'hypothèse $\mathcal{H}_1$, le gène s'exprime différentiellement, c'est à dire que les échantillons ont une distribution différente sur cette variable.

Nous fixons une p-value $p = 0.05$, sans correction en premier lieu :

```{r}
pvalue = 0.05
ttests <- list()
selected.variables <- list()
for (i in 1:p)
{
  ttests[[i]] <- t.test(Xsain[, i], Xmalade[, i])
  if (ttests[[i]]$p.value<pvalue)
  {
    selected.variables <- append(selected.variables, i)
  }
}
sprintf("Number of selected variables before correction: %d", length(selected.variables))

```

Le nombre de variables sélectionnées comme ayant une influence sur le statut de l'échantillon est assez élevé : il y a sûrement un certain nombre de faux positifs étant donné le grand nombre de tests. On applique donc une correction de Bonferroni en divisant notre pvalue par le nombre de tests que l'on effectue :

```{r}
pvalue = 0.05/p
selected.variables <- list()
for (i in 1:p)
{
  if (ttests[[i]]$p.value<pvalue)
  {
    selected.variables <- append(selected.variables, i)
  }
}
sprintf("Number of selected variables after correction: %d", length(selected.variables))
```
On a un nombre de variables sélectionnées beaucoup plus restreint, au prix d'avoir généré sans doute davantage de faux négatifs.

## Question 3.

<!-- $cov(\textbf{Xa}, \textbf{y}) = \textbf{X}cov(\textbf{a}, \textbf{y}) = \textbf{X}\mathbb{E}[\textbf{ay}^T]$ car $y$ est centrée. -->
<!-- $cov(\textbf{Xa}, \textbf{y}) = \frac{1}{n}\langle \textbf{Xa}\mid \textbf{y}\rangle = \frac{1}{n}\langle \textbf{X}^T\textbf{y}\mid \textbf{a}\rangle$ -->
<!-- Et on sait que $\nabla_{\textbf{a}} cov(\textbf{Xa}, \textbf{y}) = \frac{1}{n}\textbf{X}^T\textbf{y}$. -->

<!-- Ce problème est équivalent à la minimisation de $-cov(\textbf{Xa}, \textbf{y})$ selon $\textbf{a}$, avec  $\textbf{a}$ dans un compact (intersection de fermés bornés en dimension finie). Or toute fonction continue sur un compact atteint ses bornes, donc le problème admet une solution $\textbf{a}^*$ (on ne sait pas si elle est unique). -->
<!-- En notant $\textbf{b} = \frac{1}{n}\textbf{X}^T\textbf{y}$, le Lagrangien du problème s'écrit  :  -->
<!-- $\mathcal{L}(\textbf{a}, \lambda, \mu) = -\textbf{b}^T\textbf{a} + \lambda(\sum_{j = 1}^{p}|\textbf{a}_j| - s) + \frac{\mu}{2}(\langle \textbf{a}\mid \textbf{a}\rangle - 1)$, avec $\lambda > 0$. -->
<!-- La fonction valeur absolue n'est pas différentiable en $0$, mais on peut considérer son sous-gradient, qui est $\{1\}$ pour $x>0$, $\{-1\}$ pour $x<0$ et $[-1, 1]$ pour $x = 0$. Soit $s_i$ le sous gradient de la fonction valeur absolue évaluée en $a_i$. -->

<!-- Une condition nécessaire que doit respecter la solution est : -->

<!-- $\forall i \in [| 1, p|], 0 \in \partial_{a_i} \mathcal{L}(\textbf{a}, \lambda, \mu) = -b_i + \lambda s_i +\mu a_i$ -->
<!-- Supposons $\mu \neq 0$. -->


<!-- ### Cas 1: $\mid \frac{b_i}{\mu}\mid > \mid\frac{\lambda}{\mu}\mid$ -->


<!-- $\nabla_{\mu} \mathcal{L}(\textbf{a}, \lambda. \mu) = 0 \Longrightarrow 2\mu\textbf{a} = 1$ -->
<!-- $\nabla_{\textbf{a}} \mathcal{L}(\textbf{a}, \lambda. \mu) = \frac{1}{n}\textbf{X}^T\textbf{y} + \lambda signe(\textbf{a}) + 2\mu\textbf{a}$ -->
<!-- $\lambda >0$ -->
<!-- $\frac{1}{n}\textbf{X}^T\textbf{y} + \lambda \frac{|a_i|}{a_i} = 0$ -->
<!-- $\frac{1}{n}[\textbf{X}^T\textbf{y}]_i + \lambda signe(a_i) = 0$ -->
<!-- Si $signe(\frac{1}{n}[\textbf{X}^T\textbf{y}]_i) = signe(a_i)$ pas de sol -->
<!-- Si  -->

<!-- $\frac{1}{n}\sum_{i = 1}^{n}[\textbf{Xa}]_i\textbf{y}_i = \frac{1}{n}\sum_{i = 1}^{n}\sum_{j = 1}^{p}\textbf{X}_{i, j}\textbf{a}_j\textbf{y}_i$ -->

## Question 8

On construit un modèle de régression logistique parcimonieuse via le package `glmnet`.
La fonction `cv.glmnet` permet de détermniner la meilleure valeur du coefficient `lambda` par validation croisée (plus précisément, selon la procédure "leave one out" afin d'éviter tout effet aléatoire). 
On choisit pour `lambda` la valeur `lambda.min`, qui minimise l'erreur sur la validation croisée
```{r}
fit.logreg = cv.glmnet(X, y, family = "binomial", alpha = 1, nfolds = n, grouped = FALSE)
best_lambda = fit.logreg$lambda.min
best_lambda_index = findInterval(1-best_lambda, 1-fit.logreg$lambda)
```

Le nombre de variables sélectionnées est obtenu via l'attribut `glmnet.fit$df` :
```{r}
fit.logreg$glmnet.fit$df[best_lambda_index]
```
